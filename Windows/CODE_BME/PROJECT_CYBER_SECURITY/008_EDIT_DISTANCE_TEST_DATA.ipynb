{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18af120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "\n",
    "def auto_config(test_data_len):\n",
    "    total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "    total_cores = multiprocessing.cpu_count()\n",
    "    max_ram_mb = int((total_ram_gb * 1024 * 0.8) / total_cores)\n",
    "    process_workers = max(1, total_cores - 1)\n",
    "\n",
    "    if test_data_len <= 500:\n",
    "        test_parts = 2\n",
    "    elif test_data_len <= 1500:\n",
    "        test_parts = 4\n",
    "    elif test_data_len <= 3000:\n",
    "        test_parts = 6\n",
    "    else:\n",
    "        test_parts = 8\n",
    "\n",
    "    print(f\"⚙️ CORES={total_cores}, RAM={total_ram_gb:.2f} GB → WORKERS={process_workers}, RAM/WORKER={max_ram_mb}MB, PARTS={test_parts}\")\n",
    "    return test_parts, max_ram_mb, process_workers\n",
    "\n",
    "def read_multiline_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return [json.loads(line) for line in file]\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def chunk_list(lst, size):\n",
    "    return [lst[i:i + size] for i in range(0, len(lst), size)]\n",
    "\n",
    "def auto_batch_size(json_strs, max_ram_per_worker_mb=400):\n",
    "    avg_size = sum(len(s.encode('utf-8')) for s in json_strs[:10]) / 10\n",
    "    avg_size_mb = avg_size / 1024 / 1024\n",
    "    batch = max(1, int(max_ram_per_worker_mb / avg_size_mb))\n",
    "    print(f\"🧠 BATCH_SIZE={batch} (~{avg_size_mb:.2f}MB/string × {batch} ≈ {batch*avg_size_mb:.2f}MB)\")\n",
    "    return batch\n",
    "\n",
    "def save_row_to_csv(csv_file_path, data_row):\n",
    "    with open(csv_file_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(\",\".join(map(str, data_row)) + \"\\n\")\n",
    "\n",
    "def get_completed_row_count(csv_file_path):\n",
    "    if os.path.exists(csv_file_path):\n",
    "        with open(csv_file_path, 'r', encoding='utf-8') as f:\n",
    "            return sum(1 for _ in f)\n",
    "    return 0\n",
    "\n",
    "def split_test_data(test_strs, num_parts):\n",
    "    size = len(test_strs) // num_parts\n",
    "    return [test_strs[i * size: (i + 1) * size] for i in range(num_parts - 1)] + [test_strs[(num_parts - 1) * size:]]\n",
    "\n",
    "def compute_distance_row(train_idx, train_str, test_chunks):\n",
    "    try:\n",
    "        full_row = []\n",
    "        for chunk in test_chunks:\n",
    "            dists = [Levenshtein.distance(train_str, t) for t in chunk]\n",
    "            full_row.extend(dists)\n",
    "        return (train_idx, full_row)\n",
    "    except Exception as e:\n",
    "        return (train_idx, [])\n",
    "\n",
    "# ✅ MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "    base_path = r'C:\\Users\\KUNG_LOBSTER69\\Documents\\GitHub\\WORK\\Windows\\CODE_BME\\PROJECT_CYBER_SECURITY'\n",
    "    train_path = os.path.join(base_path, r'RESULT\\05.DATA_VALIDATION\\fold_3\\MALWARE_100_BENIGN_100\\validation_train.json')\n",
    "    malware_test_path = os.path.join(base_path, r'RESULT\\01.TRAIN_TEST_SET\\malware_test.json')\n",
    "    benign_test_path = os.path.join(base_path, r'RESULT\\01.TRAIN_TEST_SET\\benign_test.json')\n",
    "    output_path = os.path.join(base_path, r'RESULT\\08.EDIT_DISTANCE_TEST_DATA')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    matrix_csv_path = os.path.join(output_path, 'EDIT_DISTANCE_MATRIX_FINAL.csv')\n",
    "\n",
    "    print(\"📥 Loading data...\")\n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    malware_test = read_multiline_json(malware_test_path)\n",
    "    benign_test = read_multiline_json(benign_test_path)\n",
    "\n",
    "    if not train_data or not malware_test or not benign_test:\n",
    "        print(\"🚫 Missing train or test data.\")\n",
    "    else:\n",
    "        test_strs_full = [json.dumps(t) for t in (malware_test + benign_test)]\n",
    "\n",
    "        NUM_TEST_PARTS, MAX_RAM_PER_WORKER_MB, PROCESS_WORKERS = auto_config(len(test_strs_full))\n",
    "        test_part_list = split_test_data(test_strs_full, NUM_TEST_PARTS)\n",
    "\n",
    "        total_train = len(train_data)\n",
    "        completed_rows = get_completed_row_count(matrix_csv_path)\n",
    "\n",
    "        print(f\"📊 Train={total_train}, Test={len(test_strs_full)}, PARTS={NUM_TEST_PARTS}\")\n",
    "        print(f\"🔁 Resuming from row: {completed_rows}/{total_train}\")\n",
    "\n",
    "        for part_index, test_subset in enumerate(test_part_list):\n",
    "            print(f\"\\n🚀 PART {part_index + 1}/{NUM_TEST_PARTS} → {len(test_subset)} strings\")\n",
    "\n",
    "            BATCH_SIZE = min(auto_batch_size(test_subset, MAX_RAM_PER_WORKER_MB), 100)\n",
    "            test_chunks = chunk_list(test_subset, BATCH_SIZE)\n",
    "\n",
    "            tasks = [\n",
    "                (i, json.dumps(train_data[i]))\n",
    "                for i in range(total_train) if i >= completed_rows\n",
    "            ]\n",
    "\n",
    "            if not tasks:\n",
    "                print(f\"✅ Already completed PART {part_index + 1}\")\n",
    "                continue\n",
    "\n",
    "            with ProcessPoolExecutor(max_workers=PROCESS_WORKERS) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(compute_distance_row, train_idx, train_str, test_chunks): train_idx\n",
    "                    for train_idx, train_str in tasks\n",
    "                }\n",
    "\n",
    "                for future in tqdm(as_completed(futures), total=len(futures), desc=f\"PART {part_index + 1}\"):\n",
    "                    try:\n",
    "                        train_idx, row = future.result(timeout=300)\n",
    "                        save_row_to_csv(matrix_csv_path, row)\n",
    "                    except Exception as e:\n",
    "                        print(f\"🔥 [PART {part_index + 1}] ERROR: {e}\")\n",
    "\n",
    "        print(f\"\\n🎉 Done! Saved to: {matrix_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
