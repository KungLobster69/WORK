{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16019c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from Levenshtein import distance as lev_distance\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------\n",
    "# Parameters\n",
    "# --------------------------\n",
    "path = r\"C:\\Users\\BMEi\\Documents\\GitHub\\WORK\\Windows\\CODE_BME\\PROJECT_CYBER_SECURITY\\RESULT_01\\01.TRAIN_TEST_SET\"\n",
    "path_save = r\"C:\\Users\\BMEi\\Documents\\GitHub\\WORK\\Windows\\CODE_BME\\PROJECT_CYBER_SECURITY\\RESULT_02\\01.PROTOTYPE\"\n",
    "os.makedirs(path_save, exist_ok=True)\n",
    "\n",
    "c_benign = [100, 200, 300, 400, 500]\n",
    "c_malware = [1000, 2000, 3000, 4000, 5000]\n",
    "m_values = [2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "\n",
    "batch_size = 500\n",
    "candidate_batch_size = 5000\n",
    "num_cores = max(1, os.cpu_count() - 2)\n",
    "\n",
    "# --------------------------\n",
    "# JSON Loader\n",
    "# --------------------------\n",
    "def load_json_lines(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --------------------------\n",
    "# Helper Functions\n",
    "# --------------------------\n",
    "def pair_distance(i, j, strings_ref, prototypes_ref):\n",
    "    return (i, j, lev_distance(strings_ref[i], prototypes_ref[j]))\n",
    "\n",
    "def generate_edit_candidates(s, alphabet):\n",
    "    candidates = set()\n",
    "    for i in range(len(s)):\n",
    "        for a in alphabet:\n",
    "            if s[i] != a:\n",
    "                candidates.add(s[:i] + a + s[i+1:])\n",
    "    for i in range(len(s) + 1):\n",
    "        for a in alphabet:\n",
    "            candidates.add(s[:i] + a + s[i:])\n",
    "    for i in range(len(s)):\n",
    "        candidates.add(s[:i] + s[i+1:])\n",
    "    return candidates\n",
    "\n",
    "def compute_weighted_distance(candidate, strings, memberships):\n",
    "    return sum(m * lev_distance(candidate, x) for m, x in zip(memberships, strings))\n",
    "\n",
    "def improved_fuzzy_median_string(current_string, strings, memberships, alphabet, max_local_iter=5):\n",
    "    s = current_string\n",
    "    for _ in range(max_local_iter):\n",
    "        candidates = generate_edit_candidates(s, alphabet)\n",
    "        candidates.add(s)\n",
    "        candidates = list(candidates)\n",
    "\n",
    "        best = s\n",
    "        best_score = sum(m * lev_distance(s, x) for m, x in zip(memberships, strings))\n",
    "\n",
    "        batches = [candidates[i:i+candidate_batch_size] for i in range(0, len(candidates), candidate_batch_size)]\n",
    "        for batch in batches:\n",
    "            scores = Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "                delayed(compute_weighted_distance)(cand, strings, memberships) for cand in batch\n",
    "            )\n",
    "            min_idx = np.argmin(scores)\n",
    "            if scores[min_idx] < best_score:\n",
    "                best = batch[min_idx]\n",
    "                best_score = scores[min_idx]\n",
    "\n",
    "        if best == s:\n",
    "            break\n",
    "        s = best\n",
    "    return s\n",
    "\n",
    "def compute_distance_matrix_to_prototypes(strings, prototypes, batch_size=None, temp_save_path=None):\n",
    "    n, c = len(strings), len(prototypes)\n",
    "    if batch_size is None:\n",
    "        batch_size = 500\n",
    "    print(f\"ðŸ“ Computing Distance Matrix: strings={n} Ã— prototypes={c}\")\n",
    "    D = np.zeros((n, c), dtype=np.int32)\n",
    "\n",
    "    pairs = [(i, j) for i in range(n) for j in range(c)]\n",
    "    batches = [pairs[k:k+batch_size] for k in range(0, len(pairs), batch_size)]\n",
    "    save_every_batches = max(1, len(batches) // 100)\n",
    "\n",
    "    for batch_num, batch_pairs in enumerate(tqdm(batches, desc=\"ðŸ”§ Distance Matrix (Batched)\")):\n",
    "        results = Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "            delayed(pair_distance)(i, j, strings, prototypes) for i, j in batch_pairs\n",
    "        )\n",
    "        for i, j, d in results:\n",
    "            D[i, j] = d\n",
    "\n",
    "        if temp_save_path and ((batch_num + 1) % save_every_batches == 0 or (batch_num + 1) == len(batches)):\n",
    "            np.save(temp_save_path.replace(\".csv\", \"_distmatrix.npy\"), D)\n",
    "            print(f\"ðŸ’¾ Distance Matrix checkpoint saved at {batch_num+1}/{len(batches)}\")\n",
    "\n",
    "    return D\n",
    "\n",
    "def update_membership(D, m):\n",
    "    n, c = D.shape\n",
    "    def compute_row(i):\n",
    "        u_i = []\n",
    "        for j in range(c):\n",
    "            d_ij = D[i, j] + 1e-6\n",
    "            denom = sum((d_ij / (D[i, k] + 1e-6)) ** (1 / (m - 1)) for k in range(c))\n",
    "            u_i.append(1 / denom)\n",
    "        return u_i\n",
    "    return np.array(Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "        delayed(compute_row)(i) for i in range(n)\n",
    "    ))\n",
    "\n",
    "def compute_objective(D, U, m):\n",
    "    return np.sum((U ** m) * D)\n",
    "\n",
    "def assign_clusters(strings, prototypes):\n",
    "    def nearest(s):\n",
    "        dists = [lev_distance(s, p) for p in prototypes]\n",
    "        return np.argmin(dists)\n",
    "    return Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "        delayed(nearest)(s) for s in strings\n",
    "    )\n",
    "\n",
    "def calculate_purity(true_labels, pred_labels):\n",
    "    contingency = {}\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        contingency.setdefault(p, []).append(t)\n",
    "    majority = sum(Counter(v).most_common(1)[0][1] for v in contingency.values())\n",
    "    return majority / len(true_labels)\n",
    "\n",
    "def evaluate_clustering_quality(strings, prototypes, true_labels, save_path):\n",
    "    print(\"ðŸ“Š Evaluating clustering quality ...\")\n",
    "    pred_labels = assign_clusters(strings, prototypes)\n",
    "    purity = calculate_purity(true_labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    metrics = {\"purity\": purity, \"nmi\": nmi, \"ari\": ari}\n",
    "    with open(save_path.replace(\".csv\", \"_quality.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"âœ… Purity={purity:.4f}, NMI={nmi:.4f}, ARI={ari:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Main SG-FCMedians\n",
    "# --------------------------\n",
    "def sgfcmed_iterative_fast(strings, c, m, save_path, label, max_iter=5):\n",
    "    temp_save_path = save_path.replace(\".csv\", \"_temp.csv\")\n",
    "    idx_save_path = save_path.replace(\".csv\", \"_temp_idx.json\")\n",
    "    iter_save_path = save_path.replace(\".csv\", \"_temp_iter.json\")\n",
    "    distmatrix_save_path = save_path.replace(\".csv\", \"_distmatrix.npy\")\n",
    "\n",
    "    J_values = []\n",
    "\n",
    "    if os.path.exists(idx_save_path) and os.path.exists(iter_save_path):\n",
    "        print(\"ðŸ”„ Resuming previous run ...\")\n",
    "        with open(idx_save_path, 'r') as f:\n",
    "            prototypes_idx = json.load(f)\n",
    "        with open(iter_save_path, 'r') as f:\n",
    "            iter_info = json.load(f)\n",
    "        start_iter = iter_info['current_iter']\n",
    "        if os.path.exists(save_path.replace(\".csv\", \"_J_log.csv\")):\n",
    "            J_values = pd.read_csv(save_path.replace(\".csv\", \"_J_log.csv\"))[\"J\"].tolist()\n",
    "    else:\n",
    "        indices = list(range(len(strings)))\n",
    "        random.shuffle(indices)\n",
    "        prototypes_idx = indices[:c]\n",
    "        start_iter = 0\n",
    "        with open(idx_save_path, 'w') as f:\n",
    "            json.dump(prototypes_idx, f)\n",
    "        with open(iter_save_path, 'w') as f:\n",
    "            json.dump({'current_iter': start_iter}, f)\n",
    "\n",
    "    alphabet = set(''.join(strings))\n",
    "\n",
    "    for it in range(start_iter, max_iter):\n",
    "        print(f\"ðŸ” Iteration {it+1}/{max_iter}\")\n",
    "        prototype_strings = [strings[i] for i in prototypes_idx]\n",
    "\n",
    "        if os.path.exists(distmatrix_save_path):\n",
    "            D = np.load(distmatrix_save_path)\n",
    "        else:\n",
    "            D = compute_distance_matrix_to_prototypes(strings, prototype_strings, batch_size=batch_size, temp_save_path=temp_save_path)\n",
    "\n",
    "        U = update_membership(D, m)\n",
    "        J = compute_objective(D, U, m)\n",
    "        J_values.append(J)\n",
    "        pd.DataFrame({\"iteration\": list(range(1, len(J_values)+1)), \"J\": J_values}) \\\n",
    "            .to_csv(save_path.replace(\".csv\", \"_J_log.csv\"), index=False)\n",
    "        print(f\"ðŸ“‰ Objective J(U,P) = {J:.2f}\")\n",
    "\n",
    "        new_prototypes = [None] * c\n",
    "        completed = set()\n",
    "        if os.path.exists(temp_save_path):\n",
    "            df_temp = pd.read_csv(temp_save_path)\n",
    "            for idx, row in df_temp.iterrows():\n",
    "                if pd.notnull(row['prototype']):\n",
    "                    new_prototypes[idx] = row['prototype']\n",
    "                    completed.add(idx)\n",
    "\n",
    "        for j in tqdm(range(c), desc=f\"ðŸ§¬ Updating Prototypes (Iter {it+1})\"):\n",
    "            if j in completed:\n",
    "                continue\n",
    "            proto = improved_fuzzy_median_string(strings[prototypes_idx[j]], strings, U[:, j], alphabet)\n",
    "            new_prototypes[j] = proto\n",
    "\n",
    "            if (j+1) % max(1, c // 100) == 0 or (j+1) == c:\n",
    "                temp_df = pd.DataFrame({'prototype': new_prototypes})\n",
    "                temp_df.to_csv(temp_save_path, index=False)\n",
    "\n",
    "        prototypes_idx = []\n",
    "        for p in new_prototypes:\n",
    "            dists = [lev_distance(p, s) for s in strings]\n",
    "            prototypes_idx.append(np.argmin(dists))\n",
    "\n",
    "        with open(idx_save_path, 'w') as f:\n",
    "            json.dump(prototypes_idx, f)\n",
    "        with open(iter_save_path, 'w') as f:\n",
    "            json.dump({'current_iter': it+1}, f)\n",
    "\n",
    "        if os.path.exists(distmatrix_save_path):\n",
    "            os.remove(distmatrix_save_path)\n",
    "\n",
    "    # âœ… Plot J(U,P)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(J_values)+1), J_values, marker='o')\n",
    "    plt.title(f\"Objective J(U,P) vs Iteration (label={label}, c={c}, m={m})\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective J(U,P)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path.replace(\".csv\", \"_J_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    final_prototypes = [strings[i] for i in prototypes_idx]\n",
    "    pd.DataFrame({'prototype': final_prototypes}).to_csv(save_path, index=False)\n",
    "\n",
    "    true_labels = [0 if label == 'benign' else 1] * len(strings)\n",
    "    evaluate_clustering_quality(strings, final_prototypes, true_labels, save_path)\n",
    "\n",
    "    for f in [temp_save_path, idx_save_path, iter_save_path, distmatrix_save_path]:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "    print(f\"ðŸ Done: saved to {save_path}\")\n",
    "\n",
    "# --------------------------\n",
    "# Run Program\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ“¥ Loading benign dataset ...\")\n",
    "    benign_df = load_json_lines(os.path.join(path, \"benign_train.json\"))\n",
    "    benign_strings = benign_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    for c in c_benign:\n",
    "        for m in m_values:\n",
    "            filename = f\"benign_c{c}_m{str(m).replace('.', '_')}.csv\"\n",
    "            save_path = os.path.join(path_save, filename)\n",
    "            sgfcmed_iterative_fast(benign_strings.copy(), c, m, save_path, label=\"benign\")\n",
    "\n",
    "    del benign_df, benign_strings\n",
    "\n",
    "    print(\"ðŸ“¥ Loading malware dataset ...\")\n",
    "    malware_df = load_json_lines(os.path.join(path, \"malware_train.json\"))\n",
    "    malware_strings = malware_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    for c in c_malware:\n",
    "        for m in m_values:\n",
    "            filename = f\"malware_c{c}_m{str(m).replace('.', '_')}.csv\"\n",
    "            save_path = os.path.join(path_save, filename)\n",
    "            sgfcmed_iterative_fast(malware_strings.copy(), c, m, save_path, label=\"malware\")\n",
    "\n",
    "    del malware_df, malware_strings\n",
    "    print(\"âœ… All datasets processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
