{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16019c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# IMPORTS\n",
    "# --------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from Levenshtein import distance as lev_distance\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------\n",
    "# PARAMETERS\n",
    "# --------------------------\n",
    "path = r\"C:\\Users\\BMEi\\Documents\\GitHub\\WORK\\Windows\\CODE_BME\\PROJECT_CYBER_SECURITY\\RESULT_01\\01.TRAIN_TEST_SET\"\n",
    "path_save = r\"C:\\Users\\BMEi\\Documents\\GitHub\\WORK\\Windows\\CODE_BME\\PROJECT_CYBER_SECURITY\\RESULT_02\\01.PROTOTYPE\"\n",
    "os.makedirs(path_save, exist_ok=True)\n",
    "\n",
    "c_candidates = [100, 200, 300, 400, 500]          # candidate c values\n",
    "m_candidates = [2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0] # candidate m values\n",
    "\n",
    "batch_size = 500\n",
    "candidate_batch_size = 5000\n",
    "num_cores = max(1, os.cpu_count() - 2)\n",
    "\n",
    "# --------------------------\n",
    "# JSON Loader\n",
    "# --------------------------\n",
    "def load_json_lines(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# --------------------------\n",
    "\n",
    "def pair_distance(i, j, strings_ref, prototypes_ref):\n",
    "    return (i, j, lev_distance(strings_ref[i], prototypes_ref[j]))\n",
    "\n",
    "def generate_edit_candidates(s, alphabet):\n",
    "    candidates = set()\n",
    "    for i in range(len(s)):\n",
    "        for a in alphabet:\n",
    "            if s[i] != a:\n",
    "                candidates.add(s[:i] + a + s[i+1:])\n",
    "    for i in range(len(s) + 1):\n",
    "        for a in alphabet:\n",
    "            candidates.add(s[:i] + a + s[i:])\n",
    "    for i in range(len(s)):\n",
    "        candidates.add(s[:i] + s[i+1:])\n",
    "    return candidates\n",
    "\n",
    "def compute_weighted_distance(candidate, strings, memberships):\n",
    "    return sum(m * lev_distance(candidate, x) for m, x in zip(memberships, strings))\n",
    "\n",
    "def improved_fuzzy_median_string(current_string, strings, memberships, alphabet, max_local_iter=5):\n",
    "    s = current_string\n",
    "    for _ in range(max_local_iter):\n",
    "        candidates = generate_edit_candidates(s, alphabet)\n",
    "        candidates.add(s)\n",
    "        candidates = list(candidates)\n",
    "\n",
    "        best = s\n",
    "        best_score = sum(m * lev_distance(s, x) for m, x in zip(memberships, strings))\n",
    "\n",
    "        batches = [candidates[i:i+candidate_batch_size] for i in range(0, len(candidates), candidate_batch_size)]\n",
    "        for batch in batches:\n",
    "            scores = Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "                delayed(compute_weighted_distance)(cand, strings, memberships) for cand in batch\n",
    "            )\n",
    "            min_idx = np.argmin(scores)\n",
    "            if scores[min_idx] < best_score:\n",
    "                best = batch[min_idx]\n",
    "                best_score = scores[min_idx]\n",
    "\n",
    "        if best == s:\n",
    "            break\n",
    "        s = best\n",
    "    return s\n",
    "\n",
    "def compute_distance_matrix_to_prototypes(strings, prototypes, batch_size=None):\n",
    "    n, c = len(strings), len(prototypes)\n",
    "    if batch_size is None:\n",
    "        batch_size = 500\n",
    "    print(f\"ðŸ“ Computing Distance Matrix: strings={n} Ã— prototypes={c}\")\n",
    "    D = np.zeros((n, c), dtype=np.int32)\n",
    "\n",
    "    pairs = [(i, j) for i in range(n) for j in range(c)]\n",
    "    batches = [pairs[k:k+batch_size] for k in range(0, len(pairs), batch_size)]\n",
    "\n",
    "    for batch_num, batch_pairs in enumerate(tqdm(batches, desc=\"ðŸ”§ Distance Matrix (Batched)\")):\n",
    "        results = Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "            delayed(pair_distance)(i, j, strings, prototypes) for i, j in batch_pairs\n",
    "        )\n",
    "        for i, j, d in results:\n",
    "            D[i, j] = d\n",
    "\n",
    "    return D\n",
    "\n",
    "def update_membership(D, m):\n",
    "    n, c = D.shape\n",
    "    def compute_row(i):\n",
    "        u_i = []\n",
    "        for j in range(c):\n",
    "            d_ij = D[i, j] + 1e-6\n",
    "            denom = sum((d_ij / (D[i, k] + 1e-6)) ** (1 / (m - 1)) for k in range(c))\n",
    "            u_i.append(1 / denom)\n",
    "        return u_i\n",
    "    return np.array(Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "        delayed(compute_row)(i) for i in range(n)\n",
    "    ))\n",
    "\n",
    "def compute_objective(D, U, m):\n",
    "    return np.sum((U ** m) * D)\n",
    "\n",
    "def assign_clusters(strings, prototypes):\n",
    "    def nearest(s):\n",
    "        dists = [lev_distance(s, p) for p in prototypes]\n",
    "        return np.argmin(dists)\n",
    "    return Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "        delayed(nearest)(s) for s in strings\n",
    "    )\n",
    "\n",
    "def calculate_purity(true_labels, pred_labels):\n",
    "    contingency = {}\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        contingency.setdefault(p, []).append(t)\n",
    "    majority = sum(Counter(v).most_common(1)[0][1] for v in contingency.values())\n",
    "    return majority / len(true_labels)\n",
    "\n",
    "def evaluate_clustering_quality(strings, prototypes, true_labels, save_path):\n",
    "    print(\"ðŸ“Š Evaluating clustering quality ...\")\n",
    "    pred_labels = assign_clusters(strings, prototypes)\n",
    "    purity = calculate_purity(true_labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    metrics = {\"purity\": purity, \"nmi\": nmi, \"ari\": ari}\n",
    "    with open(save_path.replace(\".csv\", \"_quality.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"âœ… Purity={purity:.4f}, NMI={nmi:.4f}, ARI={ari:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# SGFCMedIterativeFast (Dynamic Adjust Tolerance)\n",
    "# --------------------------\n",
    "def sgfcmed_iterative_fast(strings, c, m, save_path, label, \n",
    "                           tolerance_percent=1.0, max_iter=50,\n",
    "                           adjust_every=1, adjust_rate=0.9):\n",
    "    temp_save_path = save_path.replace(\".csv\", \"_temp.csv\")\n",
    "    idx_save_path = save_path.replace(\".csv\", \"_temp_idx.json\")\n",
    "    iter_save_path = save_path.replace(\".csv\", \"_temp_iter.json\")\n",
    "\n",
    "    J_values = []\n",
    "\n",
    "    if os.path.exists(temp_save_path) and os.path.exists(idx_save_path) and os.path.exists(iter_save_path):\n",
    "        print(\"ðŸ”„ Resuming from checkpoint...\")\n",
    "        df_temp = pd.read_csv(temp_save_path)\n",
    "        with open(idx_save_path, 'r') as f:\n",
    "            prototypes_idx = json.load(f)\n",
    "        with open(iter_save_path, 'r') as f:\n",
    "            start_iter = json.load(f)['current_iter']\n",
    "        prototype_strings = df_temp['prototype'].tolist()\n",
    "        if os.path.exists(save_path.replace(\".csv\", \"_J_log.csv\")):\n",
    "            J_values = pd.read_csv(save_path.replace(\".csv\", \"_J_log.csv\"))[\"J\"].tolist()\n",
    "    else:\n",
    "        indices = list(range(len(strings)))\n",
    "        random.shuffle(indices)\n",
    "        prototypes_idx = indices[:c]\n",
    "        prototype_strings = [strings[i] for i in prototypes_idx]\n",
    "        start_iter = 0\n",
    "        pd.DataFrame({'prototype': prototype_strings}).to_csv(temp_save_path, index=False)\n",
    "        with open(idx_save_path, 'w') as f:\n",
    "            json.dump(prototypes_idx, f)\n",
    "        with open(iter_save_path, 'w') as f:\n",
    "            json.dump({'current_iter': start_iter}, f)\n",
    "\n",
    "    alphabet = set(''.join(strings))\n",
    "    tolerance = max(1, int(c * tolerance_percent / 100))\n",
    "    print(f\"âš™ï¸ Initial tolerance = {tolerance} ({tolerance_percent}% of c={c})\")\n",
    "\n",
    "    for it in range(start_iter, max_iter):\n",
    "        print(f\"ðŸ” Iteration {it+1}/{max_iter} (Current tolerance = {tolerance})\")\n",
    "\n",
    "        D = compute_distance_matrix_to_prototypes(strings, prototype_strings, batch_size=batch_size)\n",
    "        U = update_membership(D, m)\n",
    "        J = compute_objective(D, U, m)\n",
    "        J_values.append(J)\n",
    "\n",
    "        pd.DataFrame({\"iteration\": list(range(1, len(J_values)+1)), \"J\": J_values}) \\\n",
    "            .to_csv(save_path.replace(\".csv\", \"_J_log.csv\"), index=False)\n",
    "        print(f\"ðŸ“‰ Objective J(U,P) = {J:.2f}\")\n",
    "\n",
    "        new_prototypes = []\n",
    "        for j in tqdm(range(c), desc=f\"ðŸ§¬ Updating Prototypes (Iter {it+1})\"):\n",
    "            proto = improved_fuzzy_median_string(prototype_strings[j], strings, U[:, j], alphabet)\n",
    "            dists = [lev_distance(proto, s) for s in strings]\n",
    "            nearest_idx = np.argmin(dists)\n",
    "            new_prototypes.append(strings[nearest_idx])\n",
    "\n",
    "        changes = sum(1 for a, b in zip(prototype_strings, new_prototypes) if a != b)\n",
    "        print(f\"ðŸ”Ž Prototype changes: {changes}\")\n",
    "\n",
    "        pd.DataFrame({'prototype': new_prototypes}).to_csv(temp_save_path, index=False)\n",
    "        prototypes_idx = [strings.index(p) for p in new_prototypes]\n",
    "        with open(idx_save_path, 'w') as f:\n",
    "            json.dump(prototypes_idx, f)\n",
    "        with open(iter_save_path, 'w') as f:\n",
    "            json.dump({'current_iter': it+1}, f)\n",
    "\n",
    "        prototype_strings = new_prototypes\n",
    "\n",
    "        # ðŸ”¥ Dynamic Adjust Tolerance: à¸¥à¸”à¸¥à¸‡à¸—à¸¸à¸ adjust_every à¸£à¸­à¸š\n",
    "        if (it + 1) % adjust_every == 0:\n",
    "            tolerance = max(1, int(tolerance * adjust_rate))\n",
    "            print(f\"âš™ï¸ Adjusted tolerance to {tolerance}\")\n",
    "\n",
    "        if changes <= tolerance:\n",
    "            print(f\"ðŸ›‘ Stopping: prototypes converged (changes={changes} <= tolerance={tolerance}).\")\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(J_values)+1), J_values, marker='o')\n",
    "    plt.title(f\"Objective J(U,P) vs Iteration (label={label}, c={c}, m={m})\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective J(U,P)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path.replace(\".csv\", \"_J_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    final_prototypes = prototype_strings\n",
    "    pd.DataFrame({'prototype': final_prototypes}).to_csv(save_path, index=False)\n",
    "\n",
    "    true_labels = [0 if label == 'benign' else 1] * len(strings)\n",
    "    evaluate_clustering_quality(strings, final_prototypes, true_labels, save_path)\n",
    "\n",
    "    for f in [temp_save_path, idx_save_path, iter_save_path]:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "    print(f\"ðŸ Done: saved to {save_path}\")\n",
    "\n",
    "# --------------------------\n",
    "# OPTIMIZER à¸«à¸² c à¹à¸¥à¸° m à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´\n",
    "# --------------------------\n",
    "def optimizer_c_m(strings, label, c_candidates, m_candidates, save_dir,\n",
    "                  tolerance_percent=1.0, max_iter=50):\n",
    "    results = []\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for c in c_candidates:\n",
    "        for m in m_candidates:\n",
    "            print(f\"\\nðŸš€ Trying c={c}, m={m} ...\")\n",
    "            filename = f\"{label}_c{c}_m{str(m).replace('.', '_')}.csv\"\n",
    "            save_path = os.path.join(save_dir, filename)\n",
    "            sgfcmed_iterative_fast(strings.copy(), c, m, save_path, label,\n",
    "                                   tolerance_percent=tolerance_percent, max_iter=max_iter)\n",
    "\n",
    "            # Load quality metrics\n",
    "            with open(save_path.replace(\".csv\", \"_quality.json\"), 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            results.append({\n",
    "                'c': c,\n",
    "                'm': m,\n",
    "                'purity': metrics['purity'],\n",
    "                'nmi': metrics['nmi'],\n",
    "                'ari': metrics['ari'],\n",
    "                'path': save_path\n",
    "            })\n",
    "\n",
    "    # Save optimization results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(os.path.join(save_dir, f\"{label}_optimization_summary.csv\"), index=False)\n",
    "\n",
    "    # Select best c, m\n",
    "    df_sorted = df_results.sort_values(\n",
    "        by=['purity', 'nmi', 'ari'],\n",
    "        ascending=[False, False, False]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    best_c = df_sorted.loc[0, 'c']\n",
    "    best_m = df_sorted.loc[0, 'm']\n",
    "\n",
    "    print(f\"\\nðŸ† Best configuration: c={best_c}, m={best_m}\")\n",
    "    return best_c, best_m\n",
    "\n",
    "# --------------------------\n",
    "# MAIN PROGRAM (à¹€à¸¥à¸·à¸­à¸à¹ƒà¸Šà¹‰ Optimizer à¸«à¸£à¸·à¸­ Manual)\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    mode = \"optimizer\"  # à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™ \"manual\" à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¸à¸³à¸«à¸™à¸”à¹€à¸­à¸‡\n",
    "\n",
    "    print(\"ðŸ“¥ Loading benign dataset ...\")\n",
    "    benign_df = load_json_lines(os.path.join(path, \"benign_train.json\"))\n",
    "    benign_strings = benign_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    if mode == \"optimizer\":\n",
    "        print(\"ðŸ” Running optimizer for benign dataset ...\")\n",
    "        optimizer_c_m(\n",
    "            strings=benign_strings,\n",
    "            label=\"benign\",\n",
    "            c_candidates=c_candidates,\n",
    "            m_candidates=m_candidates,\n",
    "            save_dir=path_save,\n",
    "            tolerance_percent=1.0,\n",
    "            max_iter=50\n",
    "        )\n",
    "    else:\n",
    "        for c in c_candidates:\n",
    "            for m in m_candidates:\n",
    "                filename = f\"benign_c{c}_m{str(m).replace('.', '_')}.csv\"\n",
    "                save_path = os.path.join(path_save, filename)\n",
    "                sgfcmed_iterative_fast(benign_strings.copy(), c, m, save_path, label=\"benign\",\n",
    "                                       tolerance_percent=1.0, max_iter=50)\n",
    "\n",
    "    del benign_df, benign_strings\n",
    "\n",
    "    print(\"ðŸ“¥ Loading malware dataset ...\")\n",
    "    malware_df = load_json_lines(os.path.join(path, \"malware_train.json\"))\n",
    "    malware_strings = malware_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    if mode == \"optimizer\":\n",
    "        print(\"ðŸ” Running optimizer for malware dataset ...\")\n",
    "        optimizer_c_m(\n",
    "            strings=malware_strings,\n",
    "            label=\"malware\",\n",
    "            c_candidates=c_candidates,\n",
    "            m_candidates=m_candidates,\n",
    "            save_dir=path_save,\n",
    "            tolerance_percent=0.5,\n",
    "            max_iter=50\n",
    "        )\n",
    "    else:\n",
    "        for c in c_candidates:\n",
    "            for m in m_candidates:\n",
    "                filename = f\"malware_c{c}_m{str(m).replace('.', '_')}.csv\"\n",
    "                save_path = os.path.join(path_save, filename)\n",
    "                sgfcmed_iterative_fast(malware_strings.copy(), c, m, save_path, label=\"malware\",\n",
    "                                       tolerance_percent=0.5, max_iter=50)\n",
    "\n",
    "    del malware_df, malware_strings\n",
    "    print(\"âœ… All datasets processed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
