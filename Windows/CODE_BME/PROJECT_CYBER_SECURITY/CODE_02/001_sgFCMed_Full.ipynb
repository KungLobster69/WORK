{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16019c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from Levenshtein import distance as lev_distance\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------\n",
    "# Parameters\n",
    "# --------------------------------\n",
    "path = r\"C:\\Users\\BMEi\\Documents\\GitHub\\WORK\\Windows\\CODE_BME\\PROJECT_CYBER_SECURITY\\RESULT_01\\01.TRAIN_TEST_SET\"\n",
    "path_save = r\"C:\\Users\\BMEi\\Documents\\GitHub\\WORK\\Windows\\CODE_BME\\PROJECT_CYBER_SECURITY\\RESULT_02\\01.PROTOTYPE\"\n",
    "os.makedirs(path_save, exist_ok=True)\n",
    "\n",
    "c_benign = [100, 200, 300, 400, 500]\n",
    "c_malware = [1000, 2000, 3000, 4000, 5000]\n",
    "m_values = [2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "\n",
    "# --------------------------------\n",
    "# Load Datasets\n",
    "# --------------------------------\n",
    "print(\"ðŸ“¥ Loading JSON datasets ...\")\n",
    "benign_df = pd.read_json(os.path.join(path, \"benign_train.json\"), lines=True)\n",
    "malware_df = pd.read_json(os.path.join(path, \"malware_train.json\"), lines=True)\n",
    "benign_strings = benign_df.iloc[:, 0].astype(str).tolist()\n",
    "malware_strings = malware_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "# --------------------------------\n",
    "# Global functions for safe parallel\n",
    "# --------------------------------\n",
    "def pair_distance(i, j, strings, prototypes):\n",
    "    return (i, j, lev_distance(strings[i], prototypes[j]))\n",
    "\n",
    "def generate_edit_candidates(s, alphabet):\n",
    "    candidates = set()\n",
    "    for i in range(len(s)):\n",
    "        for a in alphabet:\n",
    "            if s[i] != a:\n",
    "                candidates.add(s[:i] + a + s[i+1:])\n",
    "    for i in range(len(s) + 1):\n",
    "        for a in alphabet:\n",
    "            candidates.add(s[:i] + a + s[i:])\n",
    "    for i in range(len(s)):\n",
    "        candidates.add(s[:i] + s[i+1:])\n",
    "    return candidates\n",
    "\n",
    "def improved_fuzzy_median_string(current_string, strings, memberships, alphabet, max_local_iter=5):\n",
    "    s = current_string\n",
    "    for _ in range(max_local_iter):\n",
    "        candidates = generate_edit_candidates(s, alphabet)\n",
    "        candidates.add(s)\n",
    "        best = s\n",
    "        best_score = sum(m * lev_distance(s, x) for m, x in zip(memberships, strings))\n",
    "        for c in candidates:\n",
    "            score = sum(m * lev_distance(c, x) for m, x in zip(memberships, strings))\n",
    "            if score < best_score:\n",
    "                best, best_score = c, score\n",
    "        if best == s:\n",
    "            break\n",
    "        s = best\n",
    "    return s\n",
    "\n",
    "# --------------------------------\n",
    "# Compute Distance Matrix (strings Ã— prototypes)\n",
    "# --------------------------------\n",
    "def compute_distance_matrix_to_prototypes(strings, prototypes):\n",
    "    n, c = len(strings), len(prototypes)\n",
    "    print(f\"ðŸ“ Computing Distance Matrix: strings={n} Ã— prototypes={c}\")\n",
    "    \n",
    "    pairs = [(i, j) for i in range(n) for j in range(c)]\n",
    "\n",
    "    results = Parallel(n_jobs=-1, prefer=\"processes\", backend=\"multiprocessing\")(\n",
    "        delayed(pair_distance)(i, j, strings, prototypes) for i, j in tqdm(pairs, desc=\"ðŸ”§ Distance sâ†”p\")\n",
    "    )\n",
    "    D = np.zeros((n, c), dtype=int)\n",
    "    for i, j, d in results:\n",
    "        D[i, j] = d\n",
    "    return D\n",
    "\n",
    "# --------------------------------\n",
    "# Update Membership Matrix\n",
    "# --------------------------------\n",
    "def update_membership(D, m):\n",
    "    n, c = D.shape\n",
    "    def compute_row(i):\n",
    "        u_i = []\n",
    "        for j in range(c):\n",
    "            d_ij = D[i, j] + 1e-6\n",
    "            denom = sum((d_ij / (D[i, k] + 1e-6)) ** (2 / (m - 1)) for k in range(c))\n",
    "            u_i.append(1 / denom)\n",
    "        return u_i\n",
    "    return np.array(Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
    "        delayed(compute_row)(i) for i in range(n)\n",
    "    ))\n",
    "\n",
    "# --------------------------------\n",
    "# Cluster Assignment and Evaluation\n",
    "# --------------------------------\n",
    "def assign_clusters(strings, prototypes):\n",
    "    def nearest(s):\n",
    "        dists = [lev_distance(s, p) for p in prototypes]\n",
    "        return np.argmin(dists)\n",
    "    return Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
    "        delayed(nearest)(s) for s in strings\n",
    "    )\n",
    "\n",
    "def calculate_purity(true_labels, pred_labels):\n",
    "    contingency = {}\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        contingency.setdefault(p, []).append(t)\n",
    "    majority = sum(Counter(v).most_common(1)[0][1] for v in contingency.values())\n",
    "    return majority / len(true_labels)\n",
    "\n",
    "def evaluate_clustering_quality(strings, prototypes, true_labels, save_path):\n",
    "    print(\"ðŸ“Š Evaluating clustering quality ...\")\n",
    "    pred_labels = assign_clusters(strings, prototypes)\n",
    "    purity = calculate_purity(true_labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    metrics = {\"purity\": purity, \"nmi\": nmi, \"ari\": ari}\n",
    "    with open(save_path.replace(\".csv\", \"_quality.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"âœ… Purity={purity:.4f}, NMI={nmi:.4f}, ARI={ari:.4f}\")\n",
    "\n",
    "# --------------------------------\n",
    "# Main SG-FCMedians Process\n",
    "# --------------------------------\n",
    "def sgfcmed_iterative_fast(strings, c, m, save_path, label, max_iter=5):\n",
    "    print(f\"\\nðŸš€ SG-FCMedians: label={label}, c={c}, m={m}\")\n",
    "    n = len(strings)\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "    prototypes_idx = indices[:c]\n",
    "    alphabet = set(''.join(strings))\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        print(f\"ðŸ” Iteration {it+1}/{max_iter}\")\n",
    "        prototype_strings = [strings[i] for i in prototypes_idx]\n",
    "        D = compute_distance_matrix_to_prototypes(strings, prototype_strings)\n",
    "        U = update_membership(D, m)\n",
    "\n",
    "        with parallel_backend(\"multiprocessing\"):\n",
    "            new_prototypes = Parallel(n_jobs=4, prefer=\"processes\")(\n",
    "                delayed(improved_fuzzy_median_string)(\n",
    "                    strings[prototypes_idx[j]], strings, U[:, j], alphabet\n",
    "                ) for j in tqdm(range(c), desc=f\"ðŸ§¬ Updating Prototypes\")\n",
    "            )\n",
    "\n",
    "        # Remap fuzzy strings back to nearest string\n",
    "        prototypes_idx = []\n",
    "        for p in new_prototypes:\n",
    "            dists = [lev_distance(p, s) for s in strings]\n",
    "            prototypes_idx.append(np.argmin(dists))\n",
    "\n",
    "    final_prototypes = [strings[i] for i in prototypes_idx]\n",
    "    pd.DataFrame({'prototype': final_prototypes}).to_csv(save_path, index=False)\n",
    "    true_labels = [0 if label == 'benign' else 1] * len(strings)\n",
    "    evaluate_clustering_quality(strings, final_prototypes, true_labels, save_path)\n",
    "    print(f\"ðŸ Done: saved to {save_path}\")\n",
    "\n",
    "# --------------------------------\n",
    "# Run All Configs\n",
    "# --------------------------------\n",
    "for label, str_list, c_values in [(\"benign\", benign_strings, c_benign), (\"malware\", malware_strings, c_malware)]:\n",
    "    for c in c_values:\n",
    "        for m in m_values:\n",
    "            filename = f\"{label}_c{c}_m{str(m).replace('.', '_')}.csv\"\n",
    "            save_path = os.path.join(path_save, filename)\n",
    "            sgfcmed_iterative_fast(str_list.copy(), c, m, save_path, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
